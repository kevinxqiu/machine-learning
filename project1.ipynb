{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "from implementations_modified import *\n",
    "from impl_proj1 import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "DATA_TRAIN_PATH = 'data/train.csv' #data path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "DATA_TEST_PATH = 'data/test.csv'\n",
    "y_test, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature cleaning:\n",
    "We drop the features that don't contribute in a very high way to the model (taking into account correlation matrix results and jet_num related features)\n",
    "For the remaining features, if a -999 value is founded, we substitute it with the feature mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 1.21858528e+02  4.92398193e+01  8.11819816e+01  5.78959617e+01\n",
      "  2.37309984e+00  1.89173324e+01  1.58432217e+02  1.43760943e+00\n",
      " -1.28304708e-01  3.87074191e+01 -1.09730480e-02 -8.17107200e-03\n",
      "  4.66602072e+01 -1.95074680e-02  4.35429640e-02  4.17172345e+01\n",
      " -1.01191920e-02  2.09797178e+02  9.79176000e-01  7.30645914e+01]\n",
      "[ 1.21871729e+02  4.92583872e+01  8.11223377e+01  5.78290937e+01\n",
      "  2.37421083e+00  1.89926203e+01  1.58668286e+02  1.43928858e+00\n",
      " -1.26825167e-01  3.86940752e+01 -1.19663944e-02 -1.53522257e-02\n",
      "  4.67065833e+01 -1.88911706e-02  5.20638271e-02  4.16269376e+01\n",
      " -7.98097804e-03  2.09957809e+02  9.80251233e-01  7.32676287e+01]\n"
     ]
    }
   ],
   "source": [
    "tX_rem = dealing_with_missing_data_mean(tX)\n",
    "tX_rem = np.delete(tX_rem, [0, 3, 6, 7, 17, 18], axis=1)\n",
    "tX_test_rem = dealing_with_missing_data_mean(tX_test)\n",
    "tX_test_rem = np.delete(tX_test_rem, [0, 3, 6, 7, 17, 18], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-bf6865f2415d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtX_rem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtX_rem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtX_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX_rem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_mean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import seaborn\n",
    "\n",
    "#Correlation plot\n",
    "data_mean = tX_rem.mean(axis=0)\n",
    "data_std = tX_rem.std(axis=0)\n",
    "tX_std = standardize(tX_rem,data_mean,data_std)\n",
    "corr = np.corrcoef(tX_std.T)\n",
    "\n",
    "seaborn.heatmap(corr, cmap='viridis', vmax=1, vmin=-1)\n",
    "plt.title('Pearson crrelation coeffient for features')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'corr' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e9fa3792f90a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorr_thresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcorr_thresh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorr_thresh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcorr_thresh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorr_thresh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcorr_thresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtril\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorr_thresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corr' is not defined"
     ]
    }
   ],
   "source": [
    "#Correlation plot with threshold\n",
    "corr_thresh = np.array(corr)\n",
    "corr_thresh[(np.abs(corr_thresh) > 0.7)] = 1\n",
    "corr_thresh[(np.abs(corr_thresh) <= 0.7)] = 0\n",
    "corr_thresh = np.tril(corr_thresh)\n",
    "\n",
    "seaborn.heatmap(corr_thresh, cmap='viridis')\n",
    "plt.title('Features exhibiting high correlation')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# Cross validation accuracy and lambda results comparison\n",
    "We show train and test accuracy for all models and we tune the best lambda "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Gradient descent\n",
    "Cross validation for gradient descent, using the parameters described here"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "33604, w1=-0.21935130971808003\n",
      "Gradient Descent(1806/1999): loss=0.4070200825129153, w0=-0.2696940804033602, w1=-0.21935130971808192\n",
      "Gradient Descent(1807/1999): loss=0.4070200825129153, w0=-0.26969408040335996, w1=-0.21935130971808378\n",
      "Gradient Descent(1808/1999): loss=0.4070200825129153, w0=-0.26969408040335974, w1=-0.2193513097180856\n",
      "Gradient Descent(1809/1999): loss=0.4070200825129153, w0=-0.2696940804033595, w1=-0.21935130971808742\n",
      "Gradient Descent(1810/1999): loss=0.40702008251291527, w0=-0.2696940804033593, w1=-0.2193513097180892\n",
      "Gradient Descent(1811/1999): loss=0.40702008251291527, w0=-0.26969408040335907, w1=-0.21935130971809094\n",
      "Gradient Descent(1812/1999): loss=0.40702008251291527, w0=-0.26969408040335885, w1=-0.21935130971809266\n",
      "Gradient Descent(1813/1999): loss=0.40702008251291527, w0=-0.2696940804033586, w1=-0.21935130971809438\n",
      "Gradient Descent(1814/1999): loss=0.40702008251291527, w0=-0.2696940804033584, w1=-0.21935130971809605\n",
      "Gradient Descent(1815/1999): loss=0.40702008251291527, w0=-0.2696940804033582, w1=-0.21935130971809771\n",
      "Gradient Descent(1816/1999): loss=0.40702008251291527, w0=-0.26969408040335796, w1=-0.21935130971809935\n",
      "Gradient Descent(1817/1999): loss=0.40702008251291527, w0=-0.2696940804033578, w1=-0.21935130971810096\n",
      "Gradient Descent(1818/1999): loss=0.40702008251291527, w0=-0.26969408040335763, w1=-0.21935130971810254\n",
      "Gradient Descent(1819/1999): loss=0.40702008251291527, w0=-0.26969408040335746, w1=-0.2193513097181041\n",
      "Gradient Descent(1820/1999): loss=0.40702008251291527, w0=-0.2696940804033573, w1=-0.21935130971810562\n",
      "Gradient Descent(1821/1999): loss=0.40702008251291527, w0=-0.26969408040335713, w1=-0.21935130971810712\n",
      "Gradient Descent(1822/1999): loss=0.40702008251291527, w0=-0.26969408040335696, w1=-0.2193513097181086\n",
      "Gradient Descent(1823/1999): loss=0.40702008251291527, w0=-0.2696940804033568, w1=-0.21935130971811004\n",
      "Gradient Descent(1824/1999): loss=0.4070200825129153, w0=-0.26969408040335663, w1=-0.21935130971811145\n",
      "Gradient Descent(1825/1999): loss=0.4070200825129153, w0=-0.26969408040335646, w1=-0.21935130971811287\n",
      "Gradient Descent(1826/1999): loss=0.4070200825129153, w0=-0.2696940804033563, w1=-0.21935130971811426\n",
      "Gradient Descent(1827/1999): loss=0.4070200825129153, w0=-0.26969408040335613, w1=-0.21935130971811562\n",
      "Gradient Descent(1828/1999): loss=0.4070200825129153, w0=-0.26969408040335596, w1=-0.21935130971811695\n",
      "Gradient Descent(1829/1999): loss=0.4070200825129153, w0=-0.2696940804033558, w1=-0.21935130971811828\n",
      "Gradient Descent(1830/1999): loss=0.4070200825129153, w0=-0.26969408040335563, w1=-0.21935130971811959\n",
      "Gradient Descent(1831/1999): loss=0.4070200825129153, w0=-0.26969408040335546, w1=-0.21935130971812086\n",
      "Gradient Descent(1832/1999): loss=0.4070200825129153, w0=-0.2696940804033553, w1=-0.2193513097181221\n",
      "Gradient Descent(1833/1999): loss=0.4070200825129153, w0=-0.26969408040335513, w1=-0.21935130971812336\n",
      "Gradient Descent(1834/1999): loss=0.4070200825129153, w0=-0.26969408040335496, w1=-0.21935130971812458\n",
      "Gradient Descent(1835/1999): loss=0.4070200825129153, w0=-0.2696940804033548, w1=-0.21935130971812578\n",
      "Gradient Descent(1836/1999): loss=0.4070200825129153, w0=-0.2696940804033547, w1=-0.21935130971812697\n",
      "Gradient Descent(1837/1999): loss=0.4070200825129153, w0=-0.2696940804033546, w1=-0.21935130971812813\n",
      "Gradient Descent(1838/1999): loss=0.4070200825129154, w0=-0.26969408040335446, w1=-0.21935130971812927\n",
      "Gradient Descent(1839/1999): loss=0.4070200825129153, w0=-0.26969408040335435, w1=-0.2193513097181304\n",
      "Gradient Descent(1840/1999): loss=0.40702008251291516, w0=-0.26969408040335424, w1=-0.21935130971813152\n",
      "Gradient Descent(1841/1999): loss=0.40702008251291516, w0=-0.26969408040335413, w1=-0.2193513097181326\n",
      "Gradient Descent(1842/1999): loss=0.4070200825129153, w0=-0.269694080403354, w1=-0.21935130971813369\n",
      "Gradient Descent(1843/1999): loss=0.4070200825129153, w0=-0.2696940804033539, w1=-0.21935130971813474\n",
      "Gradient Descent(1844/1999): loss=0.4070200825129153, w0=-0.2696940804033538, w1=-0.2193513097181358\n",
      "Gradient Descent(1845/1999): loss=0.4070200825129153, w0=-0.2696940804033537, w1=-0.21935130971813682\n",
      "Gradient Descent(1846/1999): loss=0.40702008251291516, w0=-0.2696940804033536, w1=-0.21935130971813782\n",
      "Gradient Descent(1847/1999): loss=0.4070200825129153, w0=-0.26969408040335346, w1=-0.21935130971813882\n",
      "Gradient Descent(1848/1999): loss=0.4070200825129153, w0=-0.26969408040335335, w1=-0.2193513097181398\n",
      "Gradient Descent(1849/1999): loss=0.4070200825129153, w0=-0.26969408040335324, w1=-0.21935130971814076\n",
      "Gradient Descent(1850/1999): loss=0.40702008251291527, w0=-0.26969408040335313, w1=-0.2193513097181417\n",
      "Gradient Descent(1851/1999): loss=0.40702008251291527, w0=-0.269694080403353, w1=-0.21935130971814265\n",
      "Gradient Descent(1852/1999): loss=0.4070200825129153, w0=-0.2696940804033529, w1=-0.21935130971814357\n",
      "Gradient Descent(1853/1999): loss=0.40702008251291527, w0=-0.2696940804033528, w1=-0.21935130971814445\n",
      "Gradient Descent(1854/1999): loss=0.40702008251291527, w0=-0.2696940804033527, w1=-0.21935130971814534\n",
      "Gradient Descent(1855/1999): loss=0.40702008251291527, w0=-0.2696940804033526, w1=-0.21935130971814623\n",
      "Gradient Descent(1856/1999): loss=0.40702008251291527, w0=-0.26969408040335247, w1=-0.2193513097181471\n",
      "Gradient Descent(1857/1999): loss=0.40702008251291527, w0=-0.26969408040335235, w1=-0.21935130971814795\n",
      "Gradient Descent(1858/1999): loss=0.40702008251291527, w0=-0.26969408040335224, w1=-0.21935130971814878\n",
      "Gradient Descent(1859/1999): loss=0.40702008251291527, w0=-0.26969408040335213, w1=-0.21935130971814962\n",
      "Gradient Descent(1860/1999): loss=0.40702008251291527, w0=-0.269694080403352, w1=-0.21935130971815042\n",
      "Gradient Descent(1861/1999): loss=0.40702008251291527, w0=-0.2696940804033519, w1=-0.21935130971815123\n",
      "Gradient Descent(1862/1999): loss=0.40702008251291527, w0=-0.2696940804033518, w1=-0.219351309718152\n",
      "Gradient Descent(1863/1999): loss=0.40702008251291527, w0=-0.2696940804033517, w1=-0.21935130971815278\n",
      "Gradient Descent(1864/1999): loss=0.40702008251291527, w0=-0.2696940804033516, w1=-0.21935130971815353\n",
      "Gradient Descent(1865/1999): loss=0.40702008251291527, w0=-0.26969408040335147, w1=-0.21935130971815428\n",
      "Gradient Descent(1866/1999): loss=0.40702008251291527, w0=-0.26969408040335136, w1=-0.21935130971815503\n",
      "Gradient Descent(1867/1999): loss=0.40702008251291527, w0=-0.2696940804033513, w1=-0.21935130971815575\n",
      "Gradient Descent(1868/1999): loss=0.40702008251291527, w0=-0.2696940804033512, w1=-0.21935130971815647\n",
      "Gradient Descent(1869/1999): loss=0.40702008251291527, w0=-0.26969408040335113, w1=-0.21935130971815717\n",
      "Gradient Descent(1870/1999): loss=0.40702008251291527, w0=-0.2696940804033511, w1=-0.21935130971815786\n",
      "Gradient Descent(1871/1999): loss=0.40702008251291527, w0=-0.269694080403351, w1=-0.21935130971815855\n",
      "Gradient Descent(1872/1999): loss=0.40702008251291527, w0=-0.2696940804033509, w1=-0.21935130971815922\n",
      "Gradient Descent(1873/1999): loss=0.40702008251291527, w0=-0.26969408040335086, w1=-0.2193513097181599\n",
      "Gradient Descent(1874/1999): loss=0.40702008251291527, w0=-0.2696940804033508, w1=-0.21935130971816053\n",
      "Gradient Descent(1875/1999): loss=0.40702008251291527, w0=-0.26969408040335074, w1=-0.21935130971816116\n",
      "Gradient Descent(1876/1999): loss=0.40702008251291527, w0=-0.2696940804033507, w1=-0.2193513097181618\n",
      "Gradient Descent(1877/1999): loss=0.40702008251291527, w0=-0.2696940804033506, w1=-0.2193513097181624\n",
      "Gradient Descent(1878/1999): loss=0.40702008251291527, w0=-0.2696940804033505, w1=-0.21935130971816302\n",
      "Gradient Descent(1879/1999): loss=0.40702008251291527, w0=-0.26969408040335047, w1=-0.21935130971816363\n",
      "Gradient Descent(1880/1999): loss=0.40702008251291527, w0=-0.2696940804033504, w1=-0.21935130971816422\n",
      "Gradient Descent(1881/1999): loss=0.40702008251291527, w0=-0.26969408040335036, w1=-0.2193513097181648\n",
      "Gradient Descent(1882/1999): loss=0.40702008251291527, w0=-0.2696940804033503, w1=-0.21935130971816538\n",
      "Gradient Descent(1883/1999): loss=0.40702008251291527, w0=-0.26969408040335024, w1=-0.21935130971816594\n",
      "Gradient Descent(1884/1999): loss=0.40702008251291527, w0=-0.2696940804033502, w1=-0.2193513097181665\n",
      "Gradient Descent(1885/1999): loss=0.40702008251291527, w0=-0.26969408040335013, w1=-0.21935130971816705\n",
      "Gradient Descent(1886/1999): loss=0.40702008251291527, w0=-0.2696940804033501, w1=-0.21935130971816758\n",
      "Gradient Descent(1887/1999): loss=0.40702008251291516, w0=-0.26969408040335, w1=-0.2193513097181681\n",
      "Gradient Descent(1888/1999): loss=0.40702008251291516, w0=-0.26969408040334997, w1=-0.21935130971816863\n",
      "Gradient Descent(1889/1999): loss=0.40702008251291516, w0=-0.2696940804033499, w1=-0.21935130971816913\n",
      "Gradient Descent(1890/1999): loss=0.40702008251291527, w0=-0.26969408040334986, w1=-0.21935130971816963\n",
      "Gradient Descent(1891/1999): loss=0.40702008251291516, w0=-0.2696940804033498, w1=-0.21935130971817013\n",
      "Gradient Descent(1892/1999): loss=0.40702008251291527, w0=-0.26969408040334975, w1=-0.21935130971817063\n",
      "Gradient Descent(1893/1999): loss=0.40702008251291527, w0=-0.2696940804033497, w1=-0.2193513097181711\n",
      "Gradient Descent(1894/1999): loss=0.40702008251291516, w0=-0.26969408040334963, w1=-0.21935130971817157\n",
      "Gradient Descent(1895/1999): loss=0.40702008251291527, w0=-0.2696940804033496, w1=-0.21935130971817204\n",
      "Gradient Descent(1896/1999): loss=0.40702008251291516, w0=-0.2696940804033495, w1=-0.2193513097181725\n",
      "Gradient Descent(1897/1999): loss=0.40702008251291516, w0=-0.26969408040334947, w1=-0.21935130971817293\n",
      "Gradient Descent(1898/1999): loss=0.40702008251291516, w0=-0.2696940804033494, w1=-0.21935130971817338\n",
      "Gradient Descent(1899/1999): loss=0.40702008251291527, w0=-0.26969408040334936, w1=-0.21935130971817382\n",
      "Gradient Descent(1900/1999): loss=0.40702008251291527, w0=-0.2696940804033493, w1=-0.21935130971817424\n",
      "Gradient Descent(1901/1999): loss=0.40702008251291527, w0=-0.26969408040334925, w1=-0.21935130971817465\n",
      "Gradient Descent(1902/1999): loss=0.40702008251291527, w0=-0.2696940804033492, w1=-0.21935130971817507\n",
      "Gradient Descent(1903/1999): loss=0.40702008251291527, w0=-0.26969408040334913, w1=-0.21935130971817549\n",
      "Gradient Descent(1904/1999): loss=0.40702008251291527, w0=-0.2696940804033491, w1=-0.2193513097181759\n",
      "Gradient Descent(1905/1999): loss=0.40702008251291527, w0=-0.269694080403349, w1=-0.21935130971817632\n",
      "Gradient Descent(1906/1999): loss=0.40702008251291527, w0=-0.26969408040334897, w1=-0.2193513097181767\n",
      "Gradient Descent(1907/1999): loss=0.40702008251291527, w0=-0.2696940804033489, w1=-0.2193513097181771\n",
      "Gradient Descent(1908/1999): loss=0.40702008251291527, w0=-0.26969408040334886, w1=-0.21935130971817748\n",
      "Gradient Descent(1909/1999): loss=0.40702008251291527, w0=-0.2696940804033488, w1=-0.21935130971817784\n",
      "Gradient Descent(1910/1999): loss=0.40702008251291527, w0=-0.26969408040334875, w1=-0.2193513097181782\n",
      "Gradient Descent(1911/1999): loss=0.40702008251291527, w0=-0.2696940804033487, w1=-0.21935130971817857\n",
      "Gradient Descent(1912/1999): loss=0.40702008251291527, w0=-0.26969408040334863, w1=-0.21935130971817893\n",
      "Gradient Descent(1913/1999): loss=0.40702008251291527, w0=-0.2696940804033486, w1=-0.2193513097181793\n",
      "Gradient Descent(1914/1999): loss=0.40702008251291527, w0=-0.2696940804033485, w1=-0.21935130971817962\n",
      "Gradient Descent(1915/1999): loss=0.40702008251291527, w0=-0.26969408040334847, w1=-0.21935130971817995\n",
      "Gradient Descent(1916/1999): loss=0.40702008251291527, w0=-0.2696940804033484, w1=-0.2193513097181803\n",
      "Gradient Descent(1917/1999): loss=0.40702008251291527, w0=-0.26969408040334836, w1=-0.21935130971818062\n",
      "Gradient Descent(1918/1999): loss=0.40702008251291527, w0=-0.2696940804033483, w1=-0.21935130971818095\n",
      "Gradient Descent(1919/1999): loss=0.40702008251291527, w0=-0.26969408040334825, w1=-0.21935130971818126\n",
      "Gradient Descent(1920/1999): loss=0.40702008251291527, w0=-0.2696940804033482, w1=-0.21935130971818156\n",
      "Gradient Descent(1921/1999): loss=0.40702008251291527, w0=-0.26969408040334814, w1=-0.21935130971818187\n",
      "Gradient Descent(1922/1999): loss=0.40702008251291527, w0=-0.2696940804033481, w1=-0.21935130971818217\n",
      "Gradient Descent(1923/1999): loss=0.40702008251291527, w0=-0.269694080403348, w1=-0.21935130971818248\n",
      "Gradient Descent(1924/1999): loss=0.40702008251291527, w0=-0.269694080403348, w1=-0.21935130971818279\n",
      "Gradient Descent(1925/1999): loss=0.40702008251291527, w0=-0.26969408040334797, w1=-0.2193513097181831\n",
      "Gradient Descent(1926/1999): loss=0.40702008251291527, w0=-0.2696940804033479, w1=-0.21935130971818337\n",
      "Gradient Descent(1927/1999): loss=0.40702008251291527, w0=-0.2696940804033479, w1=-0.21935130971818365\n",
      "Gradient Descent(1928/1999): loss=0.40702008251291527, w0=-0.26969408040334786, w1=-0.21935130971818392\n",
      "Gradient Descent(1929/1999): loss=0.40702008251291527, w0=-0.26969408040334786, w1=-0.2193513097181842\n",
      "Gradient Descent(1930/1999): loss=0.40702008251291527, w0=-0.2696940804033478, w1=-0.21935130971818448\n",
      "Gradient Descent(1931/1999): loss=0.40702008251291527, w0=-0.2696940804033478, w1=-0.21935130971818476\n",
      "Gradient Descent(1932/1999): loss=0.40702008251291527, w0=-0.26969408040334775, w1=-0.219351309718185\n",
      "Gradient Descent(1933/1999): loss=0.40702008251291527, w0=-0.26969408040334775, w1=-0.21935130971818526\n",
      "Gradient Descent(1934/1999): loss=0.40702008251291527, w0=-0.2696940804033477, w1=-0.2193513097181855\n",
      "Gradient Descent(1935/1999): loss=0.40702008251291527, w0=-0.2696940804033477, w1=-0.21935130971818576\n",
      "Gradient Descent(1936/1999): loss=0.40702008251291527, w0=-0.2696940804033477, w1=-0.219351309718186\n",
      "Gradient Descent(1937/1999): loss=0.40702008251291527, w0=-0.26969408040334764, w1=-0.21935130971818625\n",
      "Gradient Descent(1938/1999): loss=0.40702008251291527, w0=-0.26969408040334764, w1=-0.21935130971818648\n",
      "Gradient Descent(1939/1999): loss=0.40702008251291527, w0=-0.26969408040334764, w1=-0.2193513097181867\n",
      "Gradient Descent(1940/1999): loss=0.40702008251291527, w0=-0.2696940804033476, w1=-0.21935130971818692\n",
      "Gradient Descent(1941/1999): loss=0.40702008251291527, w0=-0.2696940804033476, w1=-0.21935130971818714\n",
      "Gradient Descent(1942/1999): loss=0.40702008251291527, w0=-0.2696940804033475, w1=-0.21935130971818737\n",
      "Gradient Descent(1943/1999): loss=0.40702008251291527, w0=-0.2696940804033475, w1=-0.2193513097181876\n",
      "Gradient Descent(1944/1999): loss=0.40702008251291527, w0=-0.2696940804033475, w1=-0.2193513097181878\n",
      "Gradient Descent(1945/1999): loss=0.40702008251291527, w0=-0.26969408040334747, w1=-0.21935130971818803\n",
      "Gradient Descent(1946/1999): loss=0.40702008251291527, w0=-0.26969408040334747, w1=-0.21935130971818823\n",
      "Gradient Descent(1947/1999): loss=0.40702008251291527, w0=-0.2696940804033474, w1=-0.21935130971818842\n",
      "Gradient Descent(1948/1999): loss=0.40702008251291527, w0=-0.2696940804033474, w1=-0.21935130971818861\n",
      "Gradient Descent(1949/1999): loss=0.40702008251291527, w0=-0.26969408040334736, w1=-0.2193513097181888\n",
      "Gradient Descent(1950/1999): loss=0.40702008251291527, w0=-0.26969408040334736, w1=-0.219351309718189\n",
      "Gradient Descent(1951/1999): loss=0.40702008251291516, w0=-0.26969408040334736, w1=-0.2193513097181892\n",
      "Gradient Descent(1952/1999): loss=0.40702008251291527, w0=-0.2696940804033473, w1=-0.2193513097181894\n",
      "Gradient Descent(1953/1999): loss=0.40702008251291527, w0=-0.2696940804033473, w1=-0.21935130971818959\n",
      "Gradient Descent(1954/1999): loss=0.40702008251291527, w0=-0.2696940804033473, w1=-0.21935130971818978\n",
      "Gradient Descent(1955/1999): loss=0.40702008251291527, w0=-0.26969408040334725, w1=-0.21935130971818995\n",
      "Gradient Descent(1956/1999): loss=0.40702008251291527, w0=-0.26969408040334725, w1=-0.2193513097181901\n",
      "Gradient Descent(1957/1999): loss=0.40702008251291527, w0=-0.26969408040334725, w1=-0.21935130971819028\n",
      "Gradient Descent(1958/1999): loss=0.40702008251291516, w0=-0.26969408040334725, w1=-0.21935130971819045\n",
      "Gradient Descent(1959/1999): loss=0.40702008251291516, w0=-0.26969408040334725, w1=-0.2193513097181906\n",
      "Gradient Descent(1960/1999): loss=0.40702008251291516, w0=-0.2696940804033472, w1=-0.21935130971819078\n",
      "Gradient Descent(1961/1999): loss=0.40702008251291516, w0=-0.2696940804033472, w1=-0.21935130971819095\n",
      "Gradient Descent(1962/1999): loss=0.40702008251291527, w0=-0.2696940804033472, w1=-0.2193513097181911\n",
      "Gradient Descent(1963/1999): loss=0.40702008251291527, w0=-0.26969408040334714, w1=-0.21935130971819128\n",
      "Gradient Descent(1964/1999): loss=0.40702008251291516, w0=-0.26969408040334714, w1=-0.21935130971819145\n",
      "Gradient Descent(1965/1999): loss=0.40702008251291527, w0=-0.26969408040334714, w1=-0.2193513097181916\n",
      "Gradient Descent(1966/1999): loss=0.40702008251291527, w0=-0.26969408040334714, w1=-0.21935130971819175\n",
      "Gradient Descent(1967/1999): loss=0.40702008251291516, w0=-0.2696940804033471, w1=-0.21935130971819192\n",
      "Gradient Descent(1968/1999): loss=0.40702008251291527, w0=-0.2696940804033471, w1=-0.21935130971819206\n",
      "Gradient Descent(1969/1999): loss=0.40702008251291516, w0=-0.2696940804033471, w1=-0.2193513097181922\n",
      "Gradient Descent(1970/1999): loss=0.40702008251291516, w0=-0.269694080403347, w1=-0.21935130971819233\n",
      "Gradient Descent(1971/1999): loss=0.40702008251291516, w0=-0.269694080403347, w1=-0.21935130971819247\n",
      "Gradient Descent(1972/1999): loss=0.40702008251291516, w0=-0.269694080403347, w1=-0.2193513097181926\n",
      "Gradient Descent(1973/1999): loss=0.40702008251291516, w0=-0.269694080403347, w1=-0.21935130971819275\n",
      "Gradient Descent(1974/1999): loss=0.40702008251291516, w0=-0.269694080403347, w1=-0.2193513097181929\n",
      "Gradient Descent(1975/1999): loss=0.40702008251291516, w0=-0.26969408040334697, w1=-0.219351309718193\n",
      "Gradient Descent(1976/1999): loss=0.40702008251291516, w0=-0.26969408040334697, w1=-0.2193513097181931\n",
      "Gradient Descent(1977/1999): loss=0.40702008251291516, w0=-0.26969408040334697, w1=-0.21935130971819322\n",
      "Gradient Descent(1978/1999): loss=0.40702008251291516, w0=-0.26969408040334697, w1=-0.21935130971819333\n",
      "Gradient Descent(1979/1999): loss=0.40702008251291516, w0=-0.26969408040334697, w1=-0.21935130971819344\n",
      "Gradient Descent(1980/1999): loss=0.40702008251291516, w0=-0.26969408040334697, w1=-0.21935130971819355\n",
      "Gradient Descent(1981/1999): loss=0.40702008251291516, w0=-0.26969408040334697, w1=-0.21935130971819367\n",
      "Gradient Descent(1982/1999): loss=0.40702008251291516, w0=-0.26969408040334697, w1=-0.21935130971819378\n",
      "Gradient Descent(1983/1999): loss=0.40702008251291516, w0=-0.26969408040334697, w1=-0.2193513097181939\n",
      "Gradient Descent(1984/1999): loss=0.40702008251291516, w0=-0.2696940804033469, w1=-0.219351309718194\n",
      "Gradient Descent(1985/1999): loss=0.40702008251291516, w0=-0.2696940804033469, w1=-0.2193513097181941\n",
      "Gradient Descent(1986/1999): loss=0.40702008251291516, w0=-0.2696940804033469, w1=-0.21935130971819422\n",
      "Gradient Descent(1987/1999): loss=0.40702008251291516, w0=-0.2696940804033469, w1=-0.21935130971819433\n",
      "Gradient Descent(1988/1999): loss=0.40702008251291516, w0=-0.2696940804033469, w1=-0.21935130971819444\n",
      "Gradient Descent(1989/1999): loss=0.40702008251291516, w0=-0.2696940804033469, w1=-0.21935130971819455\n",
      "Gradient Descent(1990/1999): loss=0.40702008251291516, w0=-0.2696940804033469, w1=-0.21935130971819466\n",
      "Gradient Descent(1991/1999): loss=0.40702008251291516, w0=-0.26969408040334686, w1=-0.21935130971819478\n",
      "Gradient Descent(1992/1999): loss=0.40702008251291516, w0=-0.26969408040334686, w1=-0.2193513097181949\n",
      "Gradient Descent(1993/1999): loss=0.40702008251291516, w0=-0.26969408040334686, w1=-0.219351309718195\n",
      "Gradient Descent(1994/1999): loss=0.40702008251291516, w0=-0.26969408040334686, w1=-0.2193513097181951\n",
      "Gradient Descent(1995/1999): loss=0.40702008251291516, w0=-0.26969408040334686, w1=-0.21935130971819522\n",
      "Gradient Descent(1996/1999): loss=0.40702008251291516, w0=-0.2696940804033468, w1=-0.21935130971819533\n",
      "Gradient Descent(1997/1999): loss=0.40702008251291516, w0=-0.2696940804033468, w1=-0.21935130971819544\n",
      "Gradient Descent(1998/1999): loss=0.40702008251291516, w0=-0.2696940804033468, w1=-0.21935130971819553\n",
      "Gradient Descent(1999/1999): loss=0.40702008251291516, w0=-0.2696940804033468, w1=-0.2193513097181956\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"OUTPUT_PATH = 'sample-submission.csv'\\ncreate_csv_submission(ids_test, y_pred, OUTPUT_PATH)\""
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "gamma = 0.1\n",
    "max_iters = 2000\n",
    "initial_w = np.zeros(tX_rem.shape[1])\n",
    "\n",
    "#We standarize training data\n",
    "data_mean = tX_rem.mean(axis=0)\n",
    "data_std = tX_rem.std(axis=0)\n",
    "tX_std = standardize(tX_rem,data_mean,data_std)\n",
    "\n",
    "acc_train, acc_test, _ = cross_validation_demo(y,tX_std,gamma,max_iters,\"GD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.7075130000000001]\n[0.707408]\n"
     ]
    }
   ],
   "source": [
    "print(acc_train)\n",
    "print(acc_test)"
   ]
  },
  {
   "source": [
    "## Stochastic gradient descent"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4460993610451223, w1=-0.041417005341949004\n",
      "Gradient Descent(1807/1999): loss=0.0798269441872007, w0=-0.24515891921889738, w1=-0.04130491116736546\n",
      "Gradient Descent(1808/1999): loss=0.017770972784789583, w0=-0.24540394162914295, w1=-0.04118421911427921\n",
      "Gradient Descent(1809/1999): loss=0.3315844321384507, w0=-0.24564090598084282, w1=-0.04153613921859794\n",
      "Gradient Descent(1810/1999): loss=1.0054163190017766, w0=-0.24461833154790333, w1=-0.04105191346279383\n",
      "Gradient Descent(1811/1999): loss=0.9393586318861424, w0=-0.24411563666147926, w1=-0.04137419705465375\n",
      "Gradient Descent(1812/1999): loss=0.5587664936951541, w0=-0.24468864208107563, w1=-0.040688396135197126\n",
      "Gradient Descent(1813/1999): loss=0.8096670460900828, w0=-0.2435738778747021, w1=-0.04022537053852563\n",
      "Gradient Descent(1814/1999): loss=0.10109832065313924, w0=-0.24416216152271963, w1=-0.04028260710404983\n",
      "Gradient Descent(1815/1999): loss=0.17982277882228082, w0=-0.24465970630079034, w1=-0.040487509200792283\n",
      "Gradient Descent(1816/1999): loss=0.22697123814649378, w0=-0.24547947593321, w1=-0.040978921101826465\n",
      "Gradient Descent(1817/1999): loss=0.21229157102832136, w0=-0.24607194193874957, w1=-0.04167004357944106\n",
      "Gradient Descent(1818/1999): loss=0.09474608614154381, w0=-0.24662351208424477, w1=-0.041712626663866145\n",
      "Gradient Descent(1819/1999): loss=0.5979693564875902, w0=-0.24665281842543774, w1=-0.04179525854437307\n",
      "Gradient Descent(1820/1999): loss=0.8266849028564249, w0=-0.24592468500987497, w1=-0.04134546948456654\n",
      "Gradient Descent(1821/1999): loss=0.5811686176978492, w0=-0.24476069089040206, w1=-0.04206217879213961\n",
      "Gradient Descent(1822/1999): loss=0.10450767859086911, w0=-0.24572045056020697, w1=-0.042179171405914796\n",
      "Gradient Descent(1823/1999): loss=1.860543650654161, w0=-0.2435954525136465, w1=-0.03982681246666877\n",
      "Gradient Descent(1824/1999): loss=0.10503931076293128, w0=-0.24404928313158022, w1=-0.0397285844642055\n",
      "Gradient Descent(1825/1999): loss=0.13101011761128412, w0=-0.24446749513632562, w1=-0.03992946930464665\n",
      "Gradient Descent(1826/1999): loss=0.14566993637009945, w0=-0.2450661249137503, w1=-0.03986426743226071\n",
      "Gradient Descent(1827/1999): loss=0.15794989660806757, w0=-0.24579409502090183, w1=-0.03983030627548991\n",
      "Gradient Descent(1828/1999): loss=0.2831302446503449, w0=-0.2467228605826238, w1=-0.04022778398431698\n",
      "Gradient Descent(1829/1999): loss=0.18984456342480227, w0=-0.24748540536291153, w1=-0.04000974822681873\n",
      "Gradient Descent(1830/1999): loss=0.39183973297880426, w0=-0.24686699177626723, w1=-0.04008424888855585\n",
      "Gradient Descent(1831/1999): loss=0.5815914731341912, w0=-0.24688916989672552, w1=-0.04027737238783429\n",
      "Gradient Descent(1832/1999): loss=0.07887990433197657, w0=-0.24735955420150071, w1=-0.04006094874712574\n",
      "Gradient Descent(1833/1999): loss=0.2430860345546445, w0=-0.2476930687609902, w1=-0.03992668386239182\n",
      "Gradient Descent(1834/1999): loss=0.11685483730880535, w0=-0.24806469396503034, w1=-0.03961051634060264\n",
      "Gradient Descent(1835/1999): loss=0.4892118280208776, w0=-0.24745804067297023, w1=-0.039944314924768784\n",
      "Gradient Descent(1836/1999): loss=0.6841319456851718, w0=-0.24875076103017355, w1=-0.04180328104819416\n",
      "Gradient Descent(1837/1999): loss=0.1935943962313911, w0=-0.24913665913539057, w1=-0.04200300641251614\n",
      "Gradient Descent(1838/1999): loss=0.09416472043384967, w0=-0.2497334861444839, w1=-0.04179049958335992\n",
      "Gradient Descent(1839/1999): loss=0.14971335826124166, w0=-0.2502724699811511, w1=-0.04125857428409069\n",
      "Gradient Descent(1840/1999): loss=0.43779112108864404, w0=-0.2506310581655996, w1=-0.04118115557971001\n",
      "Gradient Descent(1841/1999): loss=0.9714952774319761, w0=-0.2487101424094948, w1=-0.04088126544455544\n",
      "Gradient Descent(1842/1999): loss=0.19060351499142653, w0=-0.2489120048315403, w1=-0.04058060496981285\n",
      "Gradient Descent(1843/1999): loss=1.073760339459218, w0=-0.24711172100895545, w1=-0.03963275564865896\n",
      "Gradient Descent(1844/1999): loss=0.6587400562101362, w0=-0.24628998484693437, w1=-0.03919607667644403\n",
      "Gradient Descent(1845/1999): loss=0.49525251484825694, w0=-0.24621765189616918, w1=-0.03950133869951659\n",
      "Gradient Descent(1846/1999): loss=0.1445585604923689, w0=-0.24673665146675725, w1=-0.039476184536518855\n",
      "Gradient Descent(1847/1999): loss=0.12828576796238722, w0=-0.2473300892083921, w1=-0.039607579093912354\n",
      "Gradient Descent(1848/1999): loss=0.31478938488639147, w0=-0.24743740006007228, w1=-0.03979612736217564\n",
      "Gradient Descent(1849/1999): loss=0.583880313323886, w0=-0.24751744812231863, w1=-0.039231232764403996\n",
      "Gradient Descent(1850/1999): loss=0.31274658288400853, w0=-0.24800925780247024, w1=-0.040740389805131325\n",
      "Gradient Descent(1851/1999): loss=0.24047551952817994, w0=-0.24817848677803783, w1=-0.04103415879877823\n",
      "Gradient Descent(1852/1999): loss=0.04355335964486183, w0=-0.24834312739028255, w1=-0.04112449594701403\n",
      "Gradient Descent(1853/1999): loss=0.2489277161960628, w0=-0.2486703246483234, w1=-0.04077343534163526\n",
      "Gradient Descent(1854/1999): loss=0.12894892943136313, w0=-0.24934864339423138, w1=-0.04256960595434128\n",
      "Gradient Descent(1855/1999): loss=0.47575393603631705, w0=-0.24918270051099095, w1=-0.043581502773905344\n",
      "Gradient Descent(1856/1999): loss=2.045455900191905, w0=-0.24169333616218014, w1=-0.03999642850276281\n",
      "Gradient Descent(1857/1999): loss=0.1300487070588455, w0=-0.2423117547116602, w1=-0.039899284373413536\n",
      "Gradient Descent(1858/1999): loss=0.2501036207678445, w0=-0.24279601554382474, w1=-0.04050889724273461\n",
      "Gradient Descent(1859/1999): loss=0.2580833699819459, w0=-0.24334018833397794, w1=-0.03969638904543169\n",
      "Gradient Descent(1860/1999): loss=0.12197691600356511, w0=-0.24399888942673148, w1=-0.03955246698354808\n",
      "Gradient Descent(1861/1999): loss=0.6558539051368064, w0=-0.2429767247143533, w1=-0.038784982205481544\n",
      "Gradient Descent(1862/1999): loss=0.33605858551452733, w0=-0.24312255667937094, w1=-0.038603903551304206\n",
      "Gradient Descent(1863/1999): loss=0.2012073433254001, w0=-0.24384744709324993, w1=-0.03914093051756606\n",
      "Gradient Descent(1864/1999): loss=0.20060576536051952, w0=-0.24410367757577384, w1=-0.03948608437957395\n",
      "Gradient Descent(1865/1999): loss=0.11467171618277347, w0=-0.24440271347709855, w1=-0.03886153697337391\n",
      "Gradient Descent(1866/1999): loss=0.13109796150032263, w0=-0.2447334320287774, w1=-0.038146886716396856\n",
      "Gradient Descent(1867/1999): loss=0.6566072208888957, w0=-0.2439725867642654, w1=-0.03812768908411822\n",
      "Gradient Descent(1868/1999): loss=0.20493044130586527, w0=-0.24473790427861755, w1=-0.03786647168465803\n",
      "Gradient Descent(1869/1999): loss=0.7804475310640406, w0=-0.24421681489612887, w1=-0.03771022834364524\n",
      "Gradient Descent(1870/1999): loss=0.1812632508739291, w0=-0.24467530010819477, w1=-0.03791207155538845\n",
      "Gradient Descent(1871/1999): loss=1.167860408513712, w0=-0.24257921017837566, w1=-0.037730076665059425\n",
      "Gradient Descent(1872/1999): loss=0.04934317392120252, w0=-0.24298181039032524, w1=-0.03772128232612426\n",
      "Gradient Descent(1873/1999): loss=0.10622644951471874, w0=-0.24334363922384228, w1=-0.037494706006313085\n",
      "Gradient Descent(1874/1999): loss=0.18125714995330824, w0=-0.24372953146725998, w1=-0.03715757952875851\n",
      "Gradient Descent(1875/1999): loss=0.6586301392593396, w0=-0.2434921222339918, w1=-0.036146185173089714\n",
      "Gradient Descent(1876/1999): loss=0.680446658866282, w0=-0.24264533906619518, w1=-0.03592340631781053\n",
      "Gradient Descent(1877/1999): loss=0.12515582569956427, w0=-0.24329920587132578, w1=-0.03584390249307373\n",
      "Gradient Descent(1878/1999): loss=0.3237667079548264, w0=-0.2441023068923587, w1=-0.03629255979043966\n",
      "Gradient Descent(1879/1999): loss=0.10935907831030603, w0=-0.24449522557502296, w1=-0.03619511618832232\n",
      "Gradient Descent(1880/1999): loss=0.5220191758646725, w0=-0.24473652346520403, w1=-0.03542205866641994\n",
      "Gradient Descent(1881/1999): loss=0.21258872023038328, w0=-0.24599946936789854, w1=-0.036065241884406775\n",
      "Gradient Descent(1882/1999): loss=0.5167107034437383, w0=-0.24610539313615656, w1=-0.03602465678185907\n",
      "Gradient Descent(1883/1999): loss=0.5749887759992509, w0=-0.24660806470028315, w1=-0.03650470468117809\n",
      "Gradient Descent(1884/1999): loss=0.49746216560069234, w0=-0.24708394694294475, w1=-0.03682940857771126\n",
      "Gradient Descent(1885/1999): loss=0.3060604216087015, w0=-0.24748735157956475, w1=-0.036696918845790814\n",
      "Gradient Descent(1886/1999): loss=0.13610927985398877, w0=-0.24793058042875493, w1=-0.036834355937130046\n",
      "Gradient Descent(1887/1999): loss=0.6180542082934389, w0=-0.24761104537741485, w1=-0.03660172094510687\n",
      "Gradient Descent(1888/1999): loss=0.5829487863390137, w0=-0.24673751772993519, w1=-0.03677592348873964\n",
      "Gradient Descent(1889/1999): loss=0.21250941152809086, w0=-0.24634563218751457, w1=-0.03647771239728545\n",
      "Gradient Descent(1890/1999): loss=0.0001299500113394394, w0=-0.24638579439385738, w1=-0.036473047577542335\n",
      "Gradient Descent(1891/1999): loss=2.2125374645254636, w0=-0.24451866546833037, w1=-0.0356108218623256\n",
      "Gradient Descent(1892/1999): loss=0.839484791725354, w0=-0.2438246931136614, w1=-0.03636246590732315\n",
      "Gradient Descent(1893/1999): loss=0.1829058111679286, w0=-0.24431021133070654, w1=-0.03623726006329271\n",
      "Gradient Descent(1894/1999): loss=0.135998194994277, w0=-0.24466275396818005, w1=-0.035949187633343885\n",
      "Gradient Descent(1895/1999): loss=0.1780103442656724, w0=-0.24531851410282335, w1=-0.03616246601164145\n",
      "Gradient Descent(1896/1999): loss=0.3938070490316037, w0=-0.2465296557512752, w1=-0.03748231441774837\n",
      "Gradient Descent(1897/1999): loss=1.1437627862432855, w0=-0.24528901022044314, w1=-0.03785275554297576\n",
      "Gradient Descent(1898/1999): loss=0.35782719637155547, w0=-0.245650139611967, w1=-0.037847658835228934\n",
      "Gradient Descent(1899/1999): loss=0.37991126997913616, w0=-0.24494744633586663, w1=-0.036838374145032965\n",
      "Gradient Descent(1900/1999): loss=0.06396831806694363, w0=-0.24544814530455358, w1=-0.036726151270861575\n",
      "Gradient Descent(1901/1999): loss=0.27074566556083934, w0=-0.2453285202898386, w1=-0.0379317574684322\n",
      "Gradient Descent(1902/1999): loss=0.10222202261557878, w0=-0.24620833913725154, w1=-0.03782710400310177\n",
      "Gradient Descent(1903/1999): loss=0.12550611572290743, w0=-0.24670790968583264, w1=-0.03768293128590662\n",
      "Gradient Descent(1904/1999): loss=0.5513839961981598, w0=-0.24596511189232095, w1=-0.0375447875620208\n",
      "Gradient Descent(1905/1999): loss=0.8474008053177143, w0=-0.24424864040575187, w1=-0.03728042448190791\n",
      "Gradient Descent(1906/1999): loss=0.511247498703988, w0=-0.24445637658043762, w1=-0.03656670167779576\n",
      "Gradient Descent(1907/1999): loss=0.23832645166457347, w0=-0.24489255142766814, w1=-0.03655496601787761\n",
      "Gradient Descent(1908/1999): loss=0.5660808887449079, w0=-0.24534159260626465, w1=-0.036514884809643676\n",
      "Gradient Descent(1909/1999): loss=0.14858576011331104, w0=-0.24573871504774827, w1=-0.036115480053123164\n",
      "Gradient Descent(1910/1999): loss=0.95122777766439, w0=-0.24424287986061693, w1=-0.036135478712989645\n",
      "Gradient Descent(1911/1999): loss=0.07486542619959118, w0=-0.24467386693997528, w1=-0.03595680070186291\n",
      "Gradient Descent(1912/1999): loss=0.2044760118129368, w0=-0.24524086849459611, w1=-0.035953370819330484\n",
      "Gradient Descent(1913/1999): loss=0.8895529125433984, w0=-0.24378186119298392, w1=-0.03473579648033739\n",
      "Gradient Descent(1914/1999): loss=0.12351536506786993, w0=-0.245130403811186, w1=-0.035384467026428126\n",
      "Gradient Descent(1915/1999): loss=0.3098980439033521, w0=-0.2459470769135914, w1=-0.0353412742207002\n",
      "Gradient Descent(1916/1999): loss=0.22339673837654864, w0=-0.24622447531383387, w1=-0.03496386202228836\n",
      "Gradient Descent(1917/1999): loss=0.6852710759934451, w0=-0.24544904316206048, w1=-0.035464960628624045\n",
      "Gradient Descent(1918/1999): loss=0.8385081234855922, w0=-0.24366059311255833, w1=-0.03504806149255747\n",
      "Gradient Descent(1919/1999): loss=0.6104303532728339, w0=-0.24365944209329854, w1=-0.03454756746794326\n",
      "Gradient Descent(1920/1999): loss=0.05640410197364509, w0=-0.243825765219366, w1=-0.03434403985845256\n",
      "Gradient Descent(1921/1999): loss=0.37494262571304504, w0=-0.24371245568349179, w1=-0.033925319992421495\n",
      "Gradient Descent(1922/1999): loss=0.21369499593525285, w0=-0.24413966295709602, w1=-0.03446108369745417\n",
      "Gradient Descent(1923/1999): loss=1.106010825872146, w0=-0.24316439757405733, w1=-0.03597505026464329\n",
      "Gradient Descent(1924/1999): loss=0.2692385340820393, w0=-0.24415992659873484, w1=-0.0362618800170679\n",
      "Gradient Descent(1925/1999): loss=0.4550746251355201, w0=-0.24448879862042389, w1=-0.03560367244918468\n",
      "Gradient Descent(1926/1999): loss=0.028182609816466548, w0=-0.24478797571512229, w1=-0.035762122103079726\n",
      "Gradient Descent(1927/1999): loss=0.226798948029173, w0=-0.24536130023452485, w1=-0.036139395113684566\n",
      "Gradient Descent(1928/1999): loss=0.24065307994508073, w0=-0.24565109754507905, w1=-0.03582638462092071\n",
      "Gradient Descent(1929/1999): loss=0.09509020968737882, w0=-0.2462701889711199, w1=-0.035733255805389885\n",
      "Gradient Descent(1930/1999): loss=0.3501057579477854, w0=-0.2462318635196308, w1=-0.03591883931278894\n",
      "Gradient Descent(1931/1999): loss=0.5807913853281425, w0=-0.24679153654106242, w1=-0.03555957172624614\n",
      "Gradient Descent(1932/1999): loss=0.8616333006816905, w0=-0.24519574089994622, w1=-0.035251423770443914\n",
      "Gradient Descent(1933/1999): loss=0.44221057494707117, w0=-0.24550117304431943, w1=-0.03634057588588767\n",
      "Gradient Descent(1934/1999): loss=0.35043401081667364, w0=-0.24639031892145696, w1=-0.03691548644646932\n",
      "Gradient Descent(1935/1999): loss=0.1331384642038705, w0=-0.24668091206544268, w1=-0.03617921958123031\n",
      "Gradient Descent(1936/1999): loss=0.047107729550219635, w0=-0.24710448222410006, w1=-0.03591910040522686\n",
      "Gradient Descent(1937/1999): loss=0.06626385213313218, w0=-0.24753315031067005, w1=-0.03623059443762985\n",
      "Gradient Descent(1938/1999): loss=0.1760178258886303, w0=-0.2482860407973369, w1=-0.035876098360678774\n",
      "Gradient Descent(1939/1999): loss=0.03899939711671623, w0=-0.24848663086118128, w1=-0.0359702558776833\n",
      "Gradient Descent(1940/1999): loss=0.28091507239005137, w0=-0.24866867456338776, w1=-0.03637497003931161\n",
      "Gradient Descent(1941/1999): loss=1.1778205026799036, w0=-0.2470742870195099, w1=-0.03577884429186388\n",
      "Gradient Descent(1942/1999): loss=0.3835801820692233, w0=-0.24749798665336767, w1=-0.03542978979978804\n",
      "Gradient Descent(1943/1999): loss=0.12567664009368254, w0=-0.24788178147364193, w1=-0.0351109427220109\n",
      "Gradient Descent(1944/1999): loss=0.5523711070000898, w0=-0.24728459794969898, w1=-0.035842596345146874\n",
      "Gradient Descent(1945/1999): loss=0.22476034606998682, w0=-0.24786154289428527, w1=-0.03614988975882065\n",
      "Gradient Descent(1946/1999): loss=0.5408889927785522, w0=-0.24816102497678116, w1=-0.041351794446098815\n",
      "Gradient Descent(1947/1999): loss=0.4814924643545227, w0=-0.2480464752035442, w1=-0.04044737971584268\n",
      "Gradient Descent(1948/1999): loss=0.15082947155836918, w0=-0.24874937861195057, w1=-0.040248797998684166\n",
      "Gradient Descent(1949/1999): loss=0.2701802540500408, w0=-0.24914570405309405, w1=-0.040862764345368244\n",
      "Gradient Descent(1950/1999): loss=0.13439594203780314, w0=-0.24985264962248283, w1=-0.040659592756698504\n",
      "Gradient Descent(1951/1999): loss=0.48303226215879097, w0=-0.2503659614521971, w1=-0.04093949441405656\n",
      "Gradient Descent(1952/1999): loss=1.079046614416089, w0=-0.2495088368169009, w1=-0.041297502467646796\n",
      "Gradient Descent(1953/1999): loss=0.39362436673612505, w0=-0.25006007495589383, w1=-0.0413637834022458\n",
      "Gradient Descent(1954/1999): loss=0.855624429972492, w0=-0.24838988603781642, w1=-0.04162119305251494\n",
      "Gradient Descent(1955/1999): loss=0.7319002491679505, w0=-0.24833956078587793, w1=-0.041714862580563804\n",
      "Gradient Descent(1956/1999): loss=0.8461999224139065, w0=-0.24760163178674985, w1=-0.04083126982089188\n",
      "Gradient Descent(1957/1999): loss=0.1467999159667639, w0=-0.24806835186091472, w1=-0.04062398570087485\n",
      "Gradient Descent(1958/1999): loss=1.5192134594653264, w0=-0.24430958085954607, w1=-0.04158731673588929\n",
      "Gradient Descent(1959/1999): loss=0.4008628453075121, w0=-0.24486723233600655, w1=-0.04198895130992102\n",
      "Gradient Descent(1960/1999): loss=0.4482274417050712, w0=-0.24570935839250982, w1=-0.04201874995715565\n",
      "Gradient Descent(1961/1999): loss=0.15085982221254488, w0=-0.24641464811186456, w1=-0.042214014675529696\n",
      "Gradient Descent(1962/1999): loss=0.23606239408730878, w0=-0.24650448519502002, w1=-0.04308623945963755\n",
      "Gradient Descent(1963/1999): loss=0.07739597071465532, w0=-0.24704261083219783, w1=-0.042554355305332324\n",
      "Gradient Descent(1964/1999): loss=0.2632295908724107, w0=-0.2469461438287975, w1=-0.042373141628665366\n",
      "Gradient Descent(1965/1999): loss=0.16010676531930032, w0=-0.2472131734443912, w1=-0.04239839383018262\n",
      "Gradient Descent(1966/1999): loss=0.27107417865836064, w0=-0.2473567237829638, w1=-0.04251034919570779\n",
      "Gradient Descent(1967/1999): loss=1.1327207613970738, w0=-0.24528563830882774, w1=-0.041628691391257036\n",
      "Gradient Descent(1968/1999): loss=0.1391979783884867, w0=-0.24581625640903973, w1=-0.041615612971184233\n",
      "Gradient Descent(1969/1999): loss=0.0034463073099345533, w0=-0.24572782978606306, w1=-0.04162599362544001\n",
      "Gradient Descent(1970/1999): loss=0.8805316735032211, w0=-0.24434288332408186, w1=-0.04015133495180539\n",
      "Gradient Descent(1971/1999): loss=0.04849826524664736, w0=-0.2448014556842643, w1=-0.03994334105932342\n",
      "Gradient Descent(1972/1999): loss=0.2257852361403714, w0=-0.24508559951244158, w1=-0.03934570585123873\n",
      "Gradient Descent(1973/1999): loss=0.41356013196662844, w0=-0.24638536001289751, w1=-0.040004314271879976\n",
      "Gradient Descent(1974/1999): loss=0.21786480629913593, w0=-0.2470898136721416, w1=-0.04038623978180622\n",
      "Gradient Descent(1975/1999): loss=0.14678304255601185, w0=-0.2474366151133547, w1=-0.04024875730660684\n",
      "Gradient Descent(1976/1999): loss=0.24331587875848745, w0=-0.24804553530943355, w1=-0.04011183191786078\n",
      "Gradient Descent(1977/1999): loss=0.30245129530189807, w0=-0.24888598981372806, w1=-0.041035169013416535\n",
      "Gradient Descent(1978/1999): loss=0.0481055241411766, w0=-0.2492534748902009, w1=-0.040955323343985287\n",
      "Gradient Descent(1979/1999): loss=0.5036156686526144, w0=-0.2483114255498888, w1=-0.040418179184063624\n",
      "Gradient Descent(1980/1999): loss=0.020581710180888068, w0=-0.2487025665089104, w1=-0.04011896065069165\n",
      "Gradient Descent(1981/1999): loss=0.1861155521893491, w0=-0.2491384763816662, w1=-0.04001978333215392\n",
      "Gradient Descent(1982/1999): loss=0.964687682640143, w0=-0.24747491309778336, w1=-0.03921924187712376\n",
      "Gradient Descent(1983/1999): loss=0.12092790725164002, w0=-0.24791218642049886, w1=-0.039282033235260125\n",
      "Gradient Descent(1984/1999): loss=0.13837723734174676, w0=-0.2484215367643248, w1=-0.03871287945376204\n",
      "Gradient Descent(1985/1999): loss=0.16732825823446865, w0=-0.24907750367180745, w1=-0.038321705691288295\n",
      "Gradient Descent(1986/1999): loss=0.13527304433518764, w0=-0.2496738298202485, w1=-0.0382941369464517\n",
      "Gradient Descent(1987/1999): loss=0.6550174157932113, w0=-0.24963070165270826, w1=-0.03745697398510918\n",
      "Gradient Descent(1988/1999): loss=0.762018670906764, w0=-0.2499067429154323, w1=-0.037860572925703995\n",
      "Gradient Descent(1989/1999): loss=1.3821943191657404, w0=-0.2469738874720913, w1=-0.03791086452634518\n",
      "Gradient Descent(1990/1999): loss=0.19484311084627737, w0=-0.24739437974929113, w1=-0.037780154974548494\n",
      "Gradient Descent(1991/1999): loss=0.2237595618245921, w0=-0.2474915540086561, w1=-0.03715881078727143\n",
      "Gradient Descent(1992/1999): loss=0.3380556346889777, w0=-0.24754748473847354, w1=-0.03799882161846883\n",
      "Gradient Descent(1993/1999): loss=0.22630181008805017, w0=-0.24806426512757426, w1=-0.03782821266526995\n",
      "Gradient Descent(1994/1999): loss=1.361238656133384, w0=-0.2470436953047273, w1=-0.03751703584411781\n",
      "Gradient Descent(1995/1999): loss=0.9408450012811308, w0=-0.24553032885614073, w1=-0.03741332038395623\n",
      "Gradient Descent(1996/1999): loss=0.3185324829077227, w0=-0.24587514046965533, w1=-0.037139730767434105\n",
      "Gradient Descent(1997/1999): loss=0.4471596299757493, w0=-0.24656755318999157, w1=-0.036969417939118496\n",
      "Gradient Descent(1998/1999): loss=0.5501219438865438, w0=-0.24721124013910878, w1=-0.03839369717675667\n",
      "Gradient Descent(1999/1999): loss=0.9788040747135398, w0=-0.24607003419284204, w1=-0.037638757181531306\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.001\n",
    "max_iters = 2000\n",
    "initial_w = np.zeros(tX_rem.shape[1])\n",
    "\n",
    "#We standarize training data\n",
    "data_mean = tX_rem.mean(axis=0)\n",
    "data_std = tX_rem.std(axis=0)\n",
    "tX_std = standardize(tX_rem,data_mean,data_std)\n",
    "\n",
    "acc_train, acc_test, _ = cross_validation_demo(y,tX_std,gamma,max_iters,\"SGD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.696678]\n[0.6965519999999998]\n"
     ]
    }
   ],
   "source": [
    "print(acc_train)\n",
    "print(acc_test)"
   ]
  },
  {
   "source": [
    "## Least squares"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We standarize training data\n",
    "data_mean = tX_rem.mean(axis=0)\n",
    "data_std = tX_rem.std(axis=0)\n",
    "tX_std = standardize(tX_rem,data_mean,data_std)\n",
    "max_iters = 0 \n",
    "gamma = 0\n",
    "\n",
    "acc_trains = []\n",
    "acc_tests = []\n",
    "degrees = [1,5,7,9]\n",
    "for degree in degrees:\n",
    "    acc_train, acc_test, _ = cross_validation_demo(y,tX_std,gamma,max_iters,\"LS\",degree)\n",
    "    acc_trains.append(acc_train)\n",
    "    acc_tests.append(acc_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.730891], [0.769138], [0.7733909999999999], [0.781133]]\n[[0.7305400000000001], [0.768772], [0.772972], [0.7807920000000002]]\n"
     ]
    }
   ],
   "source": [
    "print(acc_trains)\n",
    "print(acc_tests)"
   ]
  },
  {
   "source": [
    "## Ridge regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-199c8ef5d415>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdegrees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdegree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdegrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0macc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtX_std\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"RR\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0macc_trains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0macc_tests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MEGA/EPFL/ML/ML_course/projects/machine-learning/impl_proj1.py\u001b[0m in \u001b[0;36mcross_validation_demo\u001b[0;34m(y, x, gamma, max_iters, model, degree, i)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0macc_te_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0macc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m             \u001b[0macc_tr_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0macc_te_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MEGA/EPFL/ML/ML_course/projects/machine-learning/impl_proj1.py\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(y, x, k_indices, k, lambda_, gamma, _max_iter, model, degree)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"RR\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mx_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mx_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mridge_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0my_pred_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MEGA/EPFL/ML/ML_course/projects/machine-learning/impl_proj1.py\u001b[0m in \u001b[0;36mbuild_poly\u001b[0;34m(x, degree)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdegrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0maux\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maux\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/index_tricks.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0mobjs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#We standarize training data\n",
    "data_mean = tX_rem.mean(axis=0)\n",
    "data_std = tX_rem.std(axis=0)\n",
    "tX_std = standardize(tX_rem,data_mean,data_std)\n",
    "max_iters = 0 \n",
    "gamma = 0\n",
    "\n",
    "#Running cross validation\n",
    "acc_trains = []\n",
    "acc_tests = []\n",
    "acc_w = []\n",
    "degrees = [5,7,9]\n",
    "for i,degree in enumerate(degrees):\n",
    "    acc_train, acc_test, w = cross_validation_demo(y,tX_std,gamma,max_iters,\"RR\",degree,i)\n",
    "    acc_trains.append(acc_train)\n",
    "    acc_tests.append(acc_test)\n",
    "    acc_w.append(w)\n",
    "\n",
    "#Test accuracy and lambda\n",
    "acc_tests = np.array(acc_tests)\n",
    "num = acc_tests.shape[0]\n",
    "lambdas = np.logspace(-4, -2, 8)\n",
    "lambdas = np.tile(lambdas, num)\n",
    "\n",
    "acc_tests_flat = np.array(acc_tests).ravel()\n",
    "max = np.max(acc_tests_flat)\n",
    "idx = np.where(acc_tests_flat == max)\n",
    "lamb = lambdas[idx]\n",
    "\n",
    "print(\"Maximum accuracy: \", max)\n",
    "print(\"Corresponding lambda for max accuracy: \", lamb)\n"
   ]
  },
  {
   "source": [
    "## Logistic regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "527087840304606\n",
      "Current iteration=1800, loss=0.45197796558008607\n",
      "Current iteration=1810, loss=0.4512507699798094\n",
      "Current iteration=1820, loss=0.4505271682947914\n",
      "Current iteration=1830, loss=0.44980713189030097\n",
      "Current iteration=1840, loss=0.44909063242799413\n",
      "Current iteration=1850, loss=0.4483776418622428\n",
      "Current iteration=1860, loss=0.4476681324365175\n",
      "Current iteration=1870, loss=0.44696207667981924\n",
      "Current iteration=1880, loss=0.4462594474031624\n",
      "Current iteration=1890, loss=0.4455602176961056\n",
      "Current iteration=1900, loss=0.4448643609233331\n",
      "Current iteration=1910, loss=0.44417185072128107\n",
      "Current iteration=1920, loss=0.4434826609948137\n",
      "Current iteration=1930, loss=0.44279676591394396\n",
      "Current iteration=1940, loss=0.4421141399106003\n",
      "Current iteration=1950, loss=0.4414347576754383\n",
      "Current iteration=1960, loss=0.44075859415469737\n",
      "Current iteration=1970, loss=0.440085624547099\n",
      "Current iteration=1980, loss=0.439415824300791\n",
      "Current iteration=1990, loss=0.43874916911033035\n",
      "Current iteration=0, loss=0.6931471805599453\n",
      "Current iteration=10, loss=0.6907035795040638\n",
      "Current iteration=20, loss=0.6882808382145742\n",
      "Current iteration=30, loss=0.6858787541689022\n",
      "Current iteration=40, loss=0.6834971254511996\n",
      "Current iteration=50, loss=0.6811357508134561\n",
      "Current iteration=60, loss=0.678794429735697\n",
      "Current iteration=70, loss=0.6764729624851106\n",
      "Current iteration=80, loss=0.6741711501739469\n",
      "Current iteration=90, loss=0.6718887948160507\n",
      "Current iteration=100, loss=0.669625699381883\n",
      "Current iteration=110, loss=0.6673816678519068\n",
      "Current iteration=120, loss=0.6651565052682156\n",
      "Current iteration=130, loss=0.6629500177843006\n",
      "Current iteration=140, loss=0.6607620127128603\n",
      "Current iteration=150, loss=0.6585922985715723\n",
      "Current iteration=160, loss=0.6564406851267586\n",
      "Current iteration=170, loss=0.6543069834348894\n",
      "Current iteration=180, loss=0.6521910058818833\n",
      "Current iteration=190, loss=0.6500925662201764\n",
      "Current iteration=200, loss=0.6480114796035418\n",
      "Current iteration=210, loss=0.6459475626196534\n",
      "Current iteration=220, loss=0.6439006333204057\n",
      "Current iteration=230, loss=0.6418705112499979\n",
      "Current iteration=240, loss=0.6398570174708144\n",
      "Current iteration=250, loss=0.6378599745871286\n",
      "Current iteration=260, loss=0.6358792067666807\n",
      "Current iteration=270, loss=0.6339145397601683\n",
      "Current iteration=280, loss=0.6319658009187072\n",
      "Current iteration=290, loss=0.6300328192093234\n",
      "Current iteration=300, loss=0.628115425228533\n",
      "Current iteration=310, loss=0.6262134512140806\n",
      "Current iteration=320, loss=0.6243267310549022\n",
      "Current iteration=330, loss=0.6224551002993818\n",
      "Current iteration=340, loss=0.6205983961619763\n",
      "Current iteration=350, loss=0.6187564575282765\n",
      "Current iteration=360, loss=0.6169291249585794\n",
      "Current iteration=370, loss=0.6151162406900428\n",
      "Current iteration=380, loss=0.6133176486374944\n",
      "Current iteration=390, loss=0.6115331943929632\n",
      "Current iteration=400, loss=0.6097627252240068\n",
      "Current iteration=410, loss=0.6080060900708979\n",
      "Current iteration=420, loss=0.6062631395427415\n",
      "Current iteration=430, loss=0.6045337259125796\n",
      "Current iteration=440, loss=0.6028177031115544\n",
      "Current iteration=450, loss=0.6011149267221844\n",
      "Current iteration=460, loss=0.599425253970814\n",
      "Current iteration=470, loss=0.5977485437192925\n",
      "Current iteration=480, loss=0.5960846564559354\n",
      "Current iteration=490, loss=0.5944334542858232\n",
      "Current iteration=500, loss=0.5927948009204796\n",
      "Current iteration=510, loss=0.5911685616669885\n",
      "Current iteration=520, loss=0.5895546034165813\n",
      "Current iteration=530, loss=0.5879527946327475\n",
      "Current iteration=540, loss=0.5863630053389073\n",
      "Current iteration=550, loss=0.5847851071056781\n",
      "Current iteration=560, loss=0.5832189730377862\n",
      "Current iteration=570, loss=0.5816644777606415\n",
      "Current iteration=580, loss=0.5801214974066231\n",
      "Current iteration=590, loss=0.5785899096010967\n",
      "Current iteration=600, loss=0.5770695934482001\n",
      "Current iteration=610, loss=0.5755604295164191\n",
      "Current iteration=620, loss=0.5740622998239863\n",
      "Current iteration=630, loss=0.5725750878241211\n",
      "Current iteration=640, loss=0.5710986783901383\n",
      "Current iteration=650, loss=0.5696329578004443\n",
      "Current iteration=660, loss=0.5681778137234472\n",
      "Current iteration=670, loss=0.5667331352023893\n",
      "Current iteration=680, loss=0.5652988126401315\n",
      "Current iteration=690, loss=0.5638747377838981\n",
      "Current iteration=700, loss=0.562460803710003\n",
      "Current iteration=710, loss=0.5610569048085668\n",
      "Current iteration=720, loss=0.5596629367682445\n",
      "Current iteration=730, loss=0.5582787965609699\n",
      "Current iteration=740, loss=0.5569043824267349\n",
      "Current iteration=750, loss=0.5555395938584087\n",
      "Current iteration=760, loss=0.5541843315866125\n",
      "Current iteration=770, loss=0.5528384975646539\n",
      "Current iteration=780, loss=0.5515019949535351\n",
      "Current iteration=790, loss=0.5501747281070365\n",
      "Current iteration=800, loss=0.5488566025568904\n",
      "Current iteration=810, loss=0.5475475249980443\n",
      "Current iteration=820, loss=0.5462474032740259\n",
      "Current iteration=830, loss=0.5449561463624125\n",
      "Current iteration=840, loss=0.543673664360409\n",
      "Current iteration=850, loss=0.5423998684705431\n",
      "Current iteration=860, loss=0.541134670986478\n",
      "Current iteration=870, loss=0.5398779852789489\n",
      "Current iteration=880, loss=0.5386297257818249\n",
      "Current iteration=890, loss=0.5373898079783025\n",
      "Current iteration=900, loss=0.5361581483872289\n",
      "Current iteration=910, loss=0.5349346645495621\n",
      "Current iteration=920, loss=0.5337192750149675\n",
      "Current iteration=930, loss=0.5325118993285523\n",
      "Current iteration=940, loss=0.5313124580177424\n",
      "Current iteration=950, loss=0.5301208725792997\n",
      "Current iteration=960, loss=0.5289370654664821\n",
      "Current iteration=970, loss=0.5277609600763487\n",
      "Current iteration=980, loss=0.5265924807372098\n",
      "Current iteration=990, loss=0.5254315526962198\n",
      "Current iteration=1000, loss=0.5242781021071197\n",
      "Current iteration=1010, loss=0.5231320560181214\n",
      "Current iteration=1020, loss=0.5219933423599408\n",
      "Current iteration=1030, loss=0.5208618899339759\n",
      "Current iteration=1040, loss=0.5197376284006298\n",
      "Current iteration=1050, loss=0.5186204882677813\n",
      "Current iteration=1060, loss=0.5175104008793971\n",
      "Current iteration=1070, loss=0.5164072984042933\n",
      "Current iteration=1080, loss=0.5153111138250347\n",
      "Current iteration=1090, loss=0.5142217809269831\n",
      "Current iteration=1100, loss=0.5131392342874848\n",
      "Current iteration=1110, loss=0.5120634092651993\n",
      "Current iteration=1120, loss=0.5109942419895716\n",
      "Current iteration=1130, loss=0.5099316693504404\n",
      "Current iteration=1140, loss=0.5088756289877884\n",
      "Current iteration=1150, loss=0.5078260592816265\n",
      "Current iteration=1160, loss=0.5067828993420197\n",
      "Current iteration=1170, loss=0.5057460889992413\n",
      "Current iteration=1180, loss=0.5047155687940679\n",
      "Current iteration=1190, loss=0.5036912799682022\n",
      "Current iteration=1200, loss=0.5026731644548307\n",
      "Current iteration=1210, loss=0.5016611648693095\n",
      "Current iteration=1220, loss=0.5006552244999792\n",
      "Current iteration=1230, loss=0.49965528729910963\n",
      "Current iteration=1240, loss=0.4986612978739667\n",
      "Current iteration=1250, loss=0.49767320147800836\n",
      "Current iteration=1260, loss=0.4966909440022005\n",
      "Current iteration=1270, loss=0.49571447196645696\n",
      "Current iteration=1280, loss=0.4947437325111998\n",
      "Current iteration=1290, loss=0.4937786733890374\n",
      "Current iteration=1300, loss=0.492819242956562\n",
      "Current iteration=1310, loss=0.4918653901662625\n",
      "Current iteration=1320, loss=0.4909170645585524\n",
      "Current iteration=1330, loss=0.48997421625391163\n",
      "Current iteration=1340, loss=0.48903679594513905\n",
      "Current iteration=1350, loss=0.4881047548897177\n",
      "Current iteration=1360, loss=0.48717804490228633\n",
      "Current iteration=1370, loss=0.4862566183472214\n",
      "Current iteration=1380, loss=0.48534042813132366\n",
      "Current iteration=1390, loss=0.4844294276966099\n",
      "Current iteration=1400, loss=0.4835235710132092\n",
      "Current iteration=1410, loss=0.48262281257235967\n",
      "Current iteration=1420, loss=0.4817271073795068\n",
      "Current iteration=1430, loss=0.48083641094750085\n",
      "Current iteration=1440, loss=0.4799506792898927\n",
      "Current iteration=1450, loss=0.4790698689143258\n",
      "Current iteration=1460, loss=0.47819393681602257\n",
      "Current iteration=1470, loss=0.47732284047136664\n",
      "Current iteration=1480, loss=0.47645653783157627\n",
      "Current iteration=1490, loss=0.47559498731646804\n",
      "Current iteration=1500, loss=0.4747381478083138\n",
      "Current iteration=1510, loss=0.4738859786457821\n",
      "Current iteration=1520, loss=0.47303843961797104\n",
      "Current iteration=1530, loss=0.47219549095852353\n",
      "Current iteration=1540, loss=0.47135709333983067\n",
      "Current iteration=1550, loss=0.47052320786731594\n",
      "Current iteration=1560, loss=0.4696937960738044\n",
      "Current iteration=1570, loss=0.4688688199139708\n",
      "Current iteration=1580, loss=0.4680482417588694\n",
      "Current iteration=1590, loss=0.4672320243905417\n",
      "Current iteration=1600, loss=0.46642013099670226\n",
      "Current iteration=1610, loss=0.4656125251655005\n",
      "Current iteration=1620, loss=0.4648091708803594\n",
      "Current iteration=1630, loss=0.46401003251488515\n",
      "Current iteration=1640, loss=0.463215074827855\n",
      "Current iteration=1650, loss=0.4624242629582717\n",
      "Current iteration=1660, loss=0.4616375624204936\n",
      "Current iteration=1670, loss=0.46085493909943054\n",
      "Current iteration=1680, loss=0.46007635924581314\n",
      "Current iteration=1690, loss=0.45930178947152506\n",
      "Current iteration=1700, loss=0.45853119674500636\n",
      "Current iteration=1710, loss=0.45776454838671976\n",
      "Current iteration=1720, loss=0.45700181206468266\n",
      "Current iteration=1730, loss=0.4562429557900643\n",
      "Current iteration=1740, loss=0.45548794791284253\n",
      "Current iteration=1750, loss=0.4547367571175268\n",
      "Current iteration=1760, loss=0.45398935241893823\n",
      "Current iteration=1770, loss=0.45324570315805146\n",
      "Current iteration=1780, loss=0.4525057789978973\n",
      "Current iteration=1790, loss=0.4517695499195192\n",
      "Current iteration=1800, loss=0.4510369862179917\n",
      "Current iteration=1810, loss=0.450308058498493\n",
      "Current iteration=1820, loss=0.449582737672433\n",
      "Current iteration=1830, loss=0.44886099495363757\n",
      "Current iteration=1840, loss=0.44814280185458505\n",
      "Current iteration=1850, loss=0.44742813018269745\n",
      "Current iteration=1860, loss=0.44671695203668255\n",
      "Current iteration=1870, loss=0.446009239802929\n",
      "Current iteration=1880, loss=0.44530496615195103\n",
      "Current iteration=1890, loss=0.444604104034883\n",
      "Current iteration=1900, loss=0.44390662668002473\n",
      "Current iteration=1910, loss=0.44321250758943265\n",
      "Current iteration=1920, loss=0.44252172053556194\n",
      "Current iteration=1930, loss=0.44183423955795303\n",
      "Current iteration=1940, loss=0.4411500389599648\n",
      "Current iteration=1950, loss=0.44046909330555495\n",
      "Current iteration=1960, loss=0.4397913774161034\n",
      "Current iteration=1970, loss=0.43911686636728087\n",
      "Current iteration=1980, loss=0.438445535485961\n",
      "Current iteration=1990, loss=0.4377773603471738\n",
      "Current iteration=0, loss=0.6931471805599453\n",
      "Current iteration=10, loss=0.6906961571250323\n",
      "Current iteration=20, loss=0.6882659878615845\n",
      "Current iteration=30, loss=0.6858564712609818\n",
      "Current iteration=40, loss=0.6834674064142479\n",
      "Current iteration=50, loss=0.6810985930719637\n",
      "Current iteration=60, loss=0.6787498317032828\n",
      "Current iteration=70, loss=0.6764209235539013\n",
      "Current iteration=80, loss=0.6741116707028404\n",
      "Current iteration=90, loss=0.6718218761178926\n",
      "Current iteration=100, loss=0.6695513437096106\n",
      "Current iteration=110, loss=0.6672998783837018\n",
      "Current iteration=120, loss=0.6650672860917243\n",
      "Current iteration=130, loss=0.6628533738799766\n",
      "Current iteration=140, loss=0.6606579499364916\n",
      "Current iteration=150, loss=0.6584808236360553\n",
      "Current iteration=160, loss=0.6563218055831839\n",
      "Current iteration=170, loss=0.6541807076530063\n",
      "Current iteration=180, loss=0.6520573430300118\n",
      "Current iteration=190, loss=0.6499515262446316\n",
      "Current iteration=200, loss=0.6478630732076385\n",
      "Current iteration=210, loss=0.6457918012423624\n",
      "Current iteration=220, loss=0.6437375291147193\n",
      "Current iteration=230, loss=0.6417000770610746\n",
      "Current iteration=240, loss=0.6396792668139626\n",
      "Current iteration=250, loss=0.6376749216256903\n",
      "Current iteration=260, loss=0.6356868662898701\n",
      "Current iteration=270, loss=0.6337149271609218\n",
      "Current iteration=280, loss=0.631758932171596\n",
      "Current iteration=290, loss=0.6298187108485765\n",
      "Current iteration=300, loss=0.6278940943262172\n",
      "Current iteration=310, loss=0.6259849153584778\n",
      "Current iteration=320, loss=0.6240910083291247\n",
      "Current iteration=330, loss=0.62221220926026\n",
      "Current iteration=340, loss=0.6203483558192522\n",
      "Current iteration=350, loss=0.6184992873241308\n",
      "Current iteration=360, loss=0.6166648447475206\n",
      "Current iteration=370, loss=0.6148448707191787\n",
      "Current iteration=380, loss=0.6130392095272085\n",
      "Current iteration=390, loss=0.6112477071180127\n",
      "Current iteration=400, loss=0.609470211095056\n",
      "Current iteration=410, loss=0.6077065707165028\n",
      "Current iteration=420, loss=0.6059566368917901\n",
      "Current iteration=430, loss=0.6042202621772023\n",
      "Current iteration=440, loss=0.6024973007705038\n",
      "Current iteration=450, loss=0.6007876085046946\n",
      "Current iteration=460, loss=0.5990910428409351\n",
      "Current iteration=470, loss=0.5974074628607062\n",
      "Current iteration=480, loss=0.5957367292572483\n",
      "Current iteration=490, loss=0.5940787043263339\n",
      "Current iteration=500, loss=0.592433251956421\n",
      "Current iteration=510, loss=0.5908002376182341\n",
      "Current iteration=520, loss=0.5891795283538136\n",
      "Current iteration=530, loss=0.587570992765084\n",
      "Current iteration=540, loss=0.5859745010019692\n",
      "Current iteration=550, loss=0.584389924750105\n",
      "Current iteration=560, loss=0.5828171372181755\n",
      "Current iteration=570, loss=0.5812560131249144\n",
      "Current iteration=580, loss=0.5797064286858004\n",
      "Current iteration=590, loss=0.5781682615994788\n",
      "Current iteration=600, loss=0.5766413910339383\n",
      "Current iteration=610, loss=0.5751256976124711\n",
      "Current iteration=620, loss=0.573621063399441\n",
      "Current iteration=630, loss=0.5721273718858858\n",
      "Current iteration=640, loss=0.5706445079749769\n",
      "Current iteration=650, loss=0.5691723579673558\n",
      "Current iteration=660, loss=0.567710809546371\n",
      "Current iteration=670, loss=0.5662597517632332\n",
      "Current iteration=680, loss=0.5648190750221049\n",
      "Current iteration=690, loss=0.5633886710651448\n",
      "Current iteration=700, loss=0.5619684329575207\n",
      "Current iteration=710, loss=0.560558255072407\n",
      "Current iteration=720, loss=0.5591580330759758\n",
      "Current iteration=730, loss=0.5577676639124043\n",
      "Current iteration=740, loss=0.5563870457889003\n",
      "Current iteration=750, loss=0.5550160781607613\n",
      "Current iteration=760, loss=0.5536546617164805\n",
      "Current iteration=770, loss=0.5523026983629021\n",
      "Current iteration=780, loss=0.5509600912104429\n",
      "Current iteration=790, loss=0.5496267445583821\n",
      "Current iteration=800, loss=0.5483025638802292\n",
      "Current iteration=810, loss=0.5469874558091794\n",
      "Current iteration=820, loss=0.5456813281236578\n",
      "Current iteration=830, loss=0.5443840897329636\n",
      "Current iteration=840, loss=0.5430956506630158\n",
      "Current iteration=850, loss=0.5418159220422073\n",
      "Current iteration=860, loss=0.5405448160873724\n",
      "Current iteration=870, loss=0.5392822460898697\n",
      "Current iteration=880, loss=0.538028126401785\n",
      "Current iteration=890, loss=0.5367823724222599\n",
      "Current iteration=900, loss=0.5355449005839454\n",
      "Current iteration=910, loss=0.5343156283395839\n",
      "Current iteration=920, loss=0.5330944741487265\n",
      "Current iteration=930, loss=0.5318813574645792\n",
      "Current iteration=940, loss=0.5306761987209895\n",
      "Current iteration=950, loss=0.5294789193195665\n",
      "Current iteration=960, loss=0.528289441616941\n",
      "Current iteration=970, loss=0.5271076889121662\n",
      "Current iteration=980, loss=0.5259335854342567\n",
      "Current iteration=990, loss=0.5247670563298708\n",
      "Current iteration=1000, loss=0.5236080276511331\n",
      "Current iteration=1010, loss=0.5224564263436012\n",
      "Current iteration=1020, loss=0.5213121802343722\n",
      "Current iteration=1030, loss=0.5201752180203345\n",
      "Current iteration=1040, loss=0.5190454692565599\n",
      "Current iteration=1050, loss=0.51792286434484\n",
      "Current iteration=1060, loss=0.5168073345223624\n",
      "Current iteration=1070, loss=0.5156988118505302\n",
      "Current iteration=1080, loss=0.514597229203923\n",
      "Current iteration=1090, loss=0.5135025202593959\n",
      "Current iteration=1100, loss=0.5124146194853221\n",
      "Current iteration=1110, loss=0.5113334621309716\n",
      "Current iteration=1120, loss=0.5102589842160298\n",
      "Current iteration=1130, loss=0.5091911225202531\n",
      "Current iteration=1140, loss=0.5081298145732613\n",
      "Current iteration=1150, loss=0.5070749986444659\n",
      "Current iteration=1160, loss=0.5060266137331311\n",
      "Current iteration=1170, loss=0.5049845995585711\n",
      "Current iteration=1180, loss=0.5039488965504774\n",
      "Current iteration=1190, loss=0.5029194458393778\n",
      "Current iteration=1200, loss=0.5018961892472263\n",
      "Current iteration=1210, loss=0.5008790692781201\n",
      "Current iteration=1220, loss=0.4998680291091451\n",
      "Current iteration=1230, loss=0.49886301258134713\n",
      "Current iteration=1240, loss=0.4978639641908291\n",
      "Current iteration=1250, loss=0.4968708290799691\n",
      "Current iteration=1260, loss=0.49588355302876347\n",
      "Current iteration=1270, loss=0.4949020824462893\n",
      "Current iteration=1280, loss=0.49392636436228604\n",
      "Current iteration=1290, loss=0.49295634641885694\n",
      "Current iteration=1300, loss=0.49199197686228463\n",
      "Current iteration=1310, loss=0.49103320453496474\n",
      "Current iteration=1320, loss=0.49007997886745064\n",
      "Current iteration=1330, loss=0.48913224987061366\n",
      "Current iteration=1340, loss=0.48818996812791193\n",
      "Current iteration=1350, loss=0.4872530847877702\n",
      "Current iteration=1360, loss=0.4863215515560682\n",
      "Current iteration=1370, loss=0.4853953206887351\n",
      "Current iteration=1380, loss=0.4844743449844499\n",
      "Current iteration=1390, loss=0.4835585777774467\n",
      "Current iteration=1400, loss=0.4826479729304216\n",
      "Current iteration=1410, loss=0.48174248482754206\n",
      "Current iteration=1420, loss=0.4808420683675553\n",
      "Current iteration=1430, loss=0.47994667895699655\n",
      "Current iteration=1440, loss=0.47905627250349403\n",
      "Current iteration=1450, loss=0.4781708054091701\n",
      "Current iteration=1460, loss=0.47729023456413694\n",
      "Current iteration=1470, loss=0.4764145173400861\n",
      "Current iteration=1480, loss=0.4755436115839695\n",
      "Current iteration=1490, loss=0.47467747561177226\n",
      "Current iteration=1500, loss=0.4738160682023732\n",
      "Current iteration=1510, loss=0.4729593485914963\n",
      "Current iteration=1520, loss=0.4721072764657458\n",
      "Current iteration=1530, loss=0.471259811956731\n",
      "Current iteration=1540, loss=0.47041691563527166\n",
      "Current iteration=1550, loss=0.4695785485056891\n",
      "Current iteration=1560, loss=0.4687446720001785\n",
      "Current iteration=1570, loss=0.4679152479732632\n",
      "Current iteration=1580, loss=0.46709023869632715\n",
      "Current iteration=1590, loss=0.4662696068522259\n",
      "Current iteration=1600, loss=0.46545331552997765\n",
      "Current iteration=1610, loss=0.46464132821952625\n",
      "Current iteration=1620, loss=0.4638336088065837\n",
      "Current iteration=1630, loss=0.4630301215675437\n",
      "Current iteration=1640, loss=0.46223083116446845\n",
      "Current iteration=1650, loss=0.4614357026401496\n",
      "Current iteration=1660, loss=0.4606447014132364\n",
      "Current iteration=1670, loss=0.4598577932734369\n",
      "Current iteration=1680, loss=0.4590749443767855\n",
      "Current iteration=1690, loss=0.4582961212409791\n",
      "Current iteration=1700, loss=0.45752129074078074\n",
      "Current iteration=1710, loss=0.4567504201034864\n",
      "Current iteration=1720, loss=0.4559834769044591\n",
      "Current iteration=1730, loss=0.4552204290627251\n",
      "Current iteration=1740, loss=0.4544612448366332\n",
      "Current iteration=1750, loss=0.45370589281957624\n",
      "Current iteration=1760, loss=0.4529543419357729\n",
      "Current iteration=1770, loss=0.4522065614361094\n",
      "Current iteration=1780, loss=0.45146252089404076\n",
      "Current iteration=1790, loss=0.4507221902015492\n",
      "Current iteration=1800, loss=0.4499855395651612\n",
      "Current iteration=1810, loss=0.44925253950201977\n",
      "Current iteration=1820, loss=0.44852316083601285\n",
      "Current iteration=1830, loss=0.4477973746939553\n",
      "Current iteration=1840, loss=0.4470751525018269\n",
      "Current iteration=1850, loss=0.4463564659810609\n",
      "Current iteration=1860, loss=0.44564128714488704\n",
      "Current iteration=1870, loss=0.444929588294724\n",
      "Current iteration=1880, loss=0.44422134201662417\n",
      "Current iteration=1890, loss=0.44351652117776735\n",
      "Current iteration=1900, loss=0.4428150989230034\n",
      "Current iteration=1910, loss=0.44211704867144436\n",
      "Current iteration=1920, loss=0.44142234411310344\n",
      "Current iteration=1930, loss=0.44073095920558064\n",
      "Current iteration=1940, loss=0.4400428681707954\n",
      "Current iteration=1950, loss=0.43935804549176416\n",
      "Current iteration=1960, loss=0.4386764659094233\n",
      "Current iteration=1970, loss=0.4379981044194954\n",
      "Current iteration=1980, loss=0.4373229362693998\n",
      "Current iteration=1990, loss=0.43665093695520557\n"
     ]
    }
   ],
   "source": [
    "#We standarize training data\n",
    "data_mean = tX_rem.mean(axis=0)\n",
    "data_std = tX_rem.std(axis=0)\n",
    "tX_std = standardize(tX_rem,data_mean,data_std)\n",
    "max_iters = 2000\n",
    "gamma = 0.001\n",
    "y_tag = change_tags(y)\n",
    "\n",
    "#Cross validation\n",
    "acc_trains = []\n",
    "acc_tests = []\n",
    "degrees = [1]\n",
    "for i,degree in enumerate(degrees):\n",
    "    acc_train, acc_test, w = cross_validation_demo(y,tX_std,gamma,max_iters,\"LR\",degree,i)\n",
    "    acc_trains.append(acc_train)\n",
    "    acc_tests.append(acc_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.702844]]\n[[0.702752]]\n"
     ]
    }
   ],
   "source": [
    "print(acc_trains)\n",
    "print(acc_tests)"
   ]
  },
  {
   "source": [
    "## Penalized logistic regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Current iteration=0, loss=0.6929002387266957\n",
      "Current iteration=10, loss=0.6904424317437151\n",
      "Current iteration=20, loss=0.688005593598901\n",
      "Current iteration=30, loss=0.6855895208586357\n",
      "Current iteration=40, loss=0.6831940106982499\n",
      "Current iteration=50, loss=0.6808188609630624\n",
      "Current iteration=60, loss=0.6784638702284285\n",
      "Current iteration=70, loss=0.676128837858657\n",
      "Current iteration=80, loss=0.673813564064657\n",
      "Current iteration=90, loss=0.6715178499601816\n",
      "Current iteration=100, loss=0.6692414976165425\n",
      "Current iteration=110, loss=0.6669843101156783\n",
      "Current iteration=120, loss=0.664746091601472\n",
      "Current iteration=130, loss=0.6625266473292217\n",
      "Current iteration=140, loss=0.6603257837131723\n",
      "Current iteration=150, loss=0.6581433083720443\n",
      "Current iteration=160, loss=0.655979030172492\n",
      "Current iteration=170, loss=0.6538327592704389\n",
      "Current iteration=180, loss=0.6517043071502611\n",
      "Current iteration=190, loss=0.6495934866617801\n",
      "Current iteration=200, loss=0.6475001120550595\n",
      "Current iteration=210, loss=0.6454239990129973\n",
      "Current iteration=220, loss=0.6433649646817121\n",
      "Current iteration=230, loss=0.6413228276987514\n",
      "Current iteration=240, loss=0.6392974082191324\n",
      "Current iteration=250, loss=0.6372885279392523\n",
      "Current iteration=260, loss=0.6352960101187038\n",
      "Current iteration=270, loss=0.6333196796000415\n",
      "Current iteration=280, loss=0.6313593628265475\n",
      "Current iteration=290, loss=0.6294148878580509\n",
      "Current iteration=300, loss=0.6274860843848593\n",
      "Current iteration=310, loss=0.6255727837398638\n",
      "Current iteration=320, loss=0.6236748189088831\n",
      "Current iteration=330, loss=0.6217920245393097\n",
      "Current iteration=340, loss=0.6199242369471297\n",
      "Current iteration=350, loss=0.6180712941223819\n",
      "Current iteration=360, loss=0.6162330357331272\n",
      "Current iteration=370, loss=0.6144093031279954\n",
      "Current iteration=380, loss=0.612599939337381\n",
      "Current iteration=390, loss=0.6108047890733544\n",
      "Current iteration=400, loss=0.609023698728355\n",
      "Current iteration=410, loss=0.6072565163727353\n",
      "Current iteration=420, loss=0.6055030917512163\n",
      "Current iteration=430, loss=0.6037632762783237\n",
      "Current iteration=440, loss=0.6020369230328596\n",
      "Current iteration=450, loss=0.6003238867514733\n",
      "Current iteration=460, loss=0.598624023821389\n",
      "Current iteration=470, loss=0.5969371922723445\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-81f091d40379>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdegrees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdegree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdegrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0macc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtX_std\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"PLR\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0macc_trains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0macc_tests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MEGA/EPFL/ML/ML_course/projects/machine-learning/impl_proj1.py\u001b[0m in \u001b[0;36mcross_validation_demo\u001b[0;34m(y, x, gamma, max_iters, model, degree, i)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0macc_te_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0macc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m             \u001b[0macc_tr_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0macc_te_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MEGA/EPFL/ML/ML_course/projects/machine-learning/impl_proj1.py\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(y, x, k_indices, k, lambda_, gamma, _max_iter, model, degree)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"PLR\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression_penalized_gradient_descent_demo_MODIFIED\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_max_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0my_pred_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels_logistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0my_pred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels_logistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MEGA/EPFL/ML/ML_course/projects/machine-learning/implementations_modified.py\u001b[0m in \u001b[0;36mlogistic_regression_penalized_gradient_descent_demo_MODIFIED\u001b[0;34m(y, tx, w, _gamma, _max_iter, _lambda)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m# get loss and update w.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_by_penalized_gradient_MODIFIED\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_loss_MODIFIED\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m# log info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MEGA/EPFL/ML/ML_course/projects/machine-learning/implementations_modified.py\u001b[0m in \u001b[0;36mcalculate_loss_MODIFIED\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;34m\"\"\"compute the loss: negative log likelihood.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogaddexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#We standarize training data\n",
    "data_mean = tX_rem.mean(axis=0)\n",
    "data_std = tX_rem.std(axis=0)\n",
    "tX_std = standardize(tX_rem,data_mean,data_std)\n",
    "max_iters = 2000 \n",
    "gamma = 0.001\n",
    "y_tag = change_tags(y)\n",
    "\n",
    "#Cross validation\n",
    "acc_trains = []\n",
    "acc_tests = []\n",
    "degrees = [1]\n",
    "for i,degree in enumerate(degrees):\n",
    "    acc_train, acc_test, w = cross_validation_demo(y,tX_std,gamma,max_iters,\"PLR\",degree,i)\n",
    "    acc_trains.append(acc_train)\n",
    "    acc_tests.append(acc_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.702843, 0.7028449999999999, 0.702846, 0.7028589999999999, 0.7028589999999999, 0.7028129999999999]]\n[[0.702756, 0.702752, 0.702768, 0.7027720000000001, 0.702716, 0.702684]]\n"
     ]
    }
   ],
   "source": [
    "print(acc_trains)\n",
    "print(acc_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Maximum accuracy:  0.7027720000000001\nCorresponding lambda for max accuracy:  [0.00063096]\n"
     ]
    }
   ],
   "source": [
    "#Test lambda and accuracy\n",
    "acc_tests = np.array(acc_tests)\n",
    "num = acc_tests.shape[0]\n",
    "lambdas = np.logspace(-5, -2, 6)\n",
    "lambdas = np.tile(lambdas, num)\n",
    "\n",
    "acc_tests_flat = np.array(acc_tests).ravel()\n",
    "max = np.max(acc_tests_flat)\n",
    "idx = np.where(acc_tests_flat == max)\n",
    "lamb = lambdas[idx]\n",
    "\n",
    "print(\"Maximum accuracy: \", max)\n",
    "print(\"Corresponding lambda for max accuracy: \", lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "name": "Python 3.8.3 64-bit ('base': conda)",
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "4aa635535829e595f413464b8d4b52eacf31c3326ae9850a263ad3e53fc82e1d"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}